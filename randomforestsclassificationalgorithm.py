# -*- coding: utf-8 -*-
"""RandomForestsClassificationAlgorithm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nwku6fPeQpCOhD2luQReT0e3asn4RXeS
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from google.colab import files
import numpy as np

uploaded = files.upload()

data = pd.read_csv('train.csv')

X = data.drop('KOL', axis=1)
y = data['KOL']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set size:", len(X_train))
print("Testing set size:", len(X_test))

print("\nX_train head:")
print(X_train.head())

print("\nX_test head:")
print(X_test.head())

print("\ny_train head:")
print(y_train.head())

print("\ny_test head:")
print(y_test.head())

class TreeNode:
    def __init__(self, is_leaf=False, label=None, split_feature=None, split_threshold=None, left_child=None, right_child=None):
        self.is_leaf = is_leaf
        self.label = label
        self.split_feature = split_feature
        self.split_threshold = split_threshold
        self.left_child = left_child
        self.right_child = right_child

class RandomForest:
    def __init__(self, n_trees=10, max_depth=3, min_sample_split=2, max_features=None):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.min_sample_split = min_sample_split
        self.max_features = max_features
        self.trees = []

    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_trees):
            tree = DecisionTree(max_depth=self.max_depth, min_sample_split=self.min_sample_split, max_features=self.max_features)
            tree.fit(X, y)
            self.trees.append(tree)
        return self

    def predict(self, X):
        predictions = np.zeros((X.shape[0], len(self.trees)))
        for i, tree in enumerate(self.trees):
            predictions[:, i] = tree.predict(X)

        final_predictions = []
        for i in range(X.shape[0]):
            counts = np.bincount(predictions[i].astype(int))
            final_predictions.append(np.argmax(counts))

        return np.array(final_predictions)

class DecisionTree:
    def __init__(self, max_depth=3, min_sample_split=2, max_features=None):
        self.max_depth = max_depth
        self.min_sample_split = min_sample_split
        self.max_features = max_features

    def fit(self, X, y):
        self.n_classes = len(np.unique(y))
        self.tree = self.build_tree(X, y)
        return self

    def predict(self, X):
        return np.array([self.traverse_tree(x, self.tree) for x in X])

    def build_tree(self, X, y, depth=0):
        n_samples, n_features = X.shape
        n_labels = len(np.unique(y))

        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_sample_split):
            leaf_value = self.most_common_label(y)
            return TreeNode(is_leaf=True, label=leaf_value)

        feature_indices = self.get_random_features(n_features)
        best_feature, best_threshold = self.choose_best_split(X, y, feature_indices)

        left_indices = X[:, best_feature] < best_threshold
        X_left, y_left = X[left_indices], y[left_indices]
        X_right, y_right = X[~left_indices], y[~left_indices]

        left_child = self.build_tree(X_left, y_left, depth+1)
        right_child = self.build_tree(X_right, y_right, depth+1)
        return TreeNode(split_feature=best_feature, split_threshold=best_threshold, left_child=left_child, right_child=right_child)

    def traverse_tree(self, x, node):
        if node.is_leaf:
            return node.label
        if x[node.split_feature] < node.split_threshold:
            return self.traverse_tree(x, node.left_child)
        return self.traverse_tree(x, node.right_child)

    def get_random_features(self, n_features):
        if self.max_features is None:
            self.max_features = n_features
        return np.random.choice(n_features, self.max_features, replace=False)

    def choose_best_split(self, X, y, feature_indices):
        best_gain = -1
        split_feature, split_threshold = None, None
        for feature_index in feature_indices:
            X_column = X[:, feature_index]
            thresholds = np.unique(X_column)
            for threshold in thresholds:
                gain = self.information_gain(y, X_column, threshold)
                if gain > best_gain:
                    best_gain = gain
                    split_feature = feature_index
                    split_threshold = threshold
        return split_feature, split_threshold

    def information_gain(self, y, X_column, threshold):
        parent_entropy = self.entropy(y)
        left_indices = X_column < threshold
        right_indices = ~left_indices
        n = len(y)
        n_left, n_right = len(y[left_indices]), len(y[right_indices])
        if n_left == 0 or n_right == 0:
            return 0
        child_entropy = (n_left / n) * self.entropy(y[left_indices]) + (n_right / n) * self.entropy(y[right_indices])
        return parent_entropy - child_entropy

    def entropy(self, y):
        _, counts = np.unique(y, return_counts=True)
        probabilities = counts / len(y)
        return -np.sum(probabilities * np.log2(probabilities + 1e-7))

    def most_common_label(self, y):
        return np.argmax(np.bincount(y))

categorical_features = ['Author']
numerical_features = ['Author ID', 'Publications', 'Citations', 'Years of experience', 'h-index', 'i10-index', 'Average citations per year', 'Coauthorship Centrality', 'Affiliation Centrality']
string_array_features = ['Affiliations', 'Interests']

categorical_transformer = OneHotEncoder(handle_unknown='ignore')
numerical_transformer = StandardScaler()
string_array_transformer = OneHotEncoder(handle_unknown='ignore')

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features),
        ('str_arr', string_array_transformer, string_array_features)
    ])

X_train_scaled = preprocessor.fit_transform(X_train)
X_test_scaled = preprocessor.transform(X_test)

X_train_scaled = X_train_scaled.toarray()
y_train = np.array(y_train)
X_test_scaled = X_test_scaled.toarray()
y_test = np.array(y_test)

rf_model = RandomForest(n_trees=10, max_depth=3, min_sample_split=2)
rf_model.fit(X_train_scaled, y_train)

y_train_pred = rf_model.predict(X_train_scaled)
train_accuracy = np.mean(y_train == y_train_pred)
print(f"Training accuracy: {train_accuracy * 100:.2f}%")

y_test_pred = rf_model.predict(X_test_scaled)
test_accuracy = np.mean(y_test == y_test_pred)
print(f"Testing accuracy: {test_accuracy * 100:.2f}%")